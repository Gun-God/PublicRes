# 阅读《CLEVE-Contrastive Pre-training for Event Extraction》：

## 1. 动机
* 现有的预训练方法没有考虑建模事件特征，导致开发的事件抽取模型不能充分利用大规模无监督数据。
* 现存的事件抽取方法主要是监督范式，通常遭受数据缺乏和有限的普遍性。
## 2. 创新
* 提出CLEVE：一种利用AMR结构构建自监督信号的面向事件的对比预训练框架，更好地从大规模无监督数据和它们的语义结构(如:AMR)中学习事件知识。
## 3. 背景知识
* 事件模式归纳 ：监督的事件抽取方法不能归纳不断出现的新事件类型和论元角色，为此，Chambers and Jurafsky探索通过无监督聚类从原始文本中归纳出事件模式
* 对比学习：对比学习是学习“邻居”的相似表示和“非邻居”的不同表示，进一步广泛用于各个领域的自监督表示学习。


## 4. 方法
CLEVE的整体框架如下图，主要包含两个部分：事件语义预训练和事件结构预训练。

<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE1.png)
<br>

#### 4.1 预处理
使用自动的AMR解析器将无监督语料库中的句子解析为AMR结构。每个AMR结构是一个有向无环图，概念作为节点，语义关系作为边。此外，每个节点通常最多只对应一个词，一个多词实体将被表示为一个节点列表，这些节点通过名称和连接运算符边相连。考虑到预训练实体表示自然地有意事件论元抽取，在事件语义和结构预训练期间，合并这些列表为单个结点表示多单词实体。给定无监督语料库中的一个句子s，经过AMR解析器得到它的AMR图gs= (Vs,Es)，Vs是单词合并之后的结点集，Es为边集，![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE2.png) ，其中R为定义的语义关系类型集合。

#### 4.2 事件语义预训练
采用一个预训练语言模型作为文本编码器并对其进行训练，目的是区分各种触发词-论元对。
###### 4.2.1 文本编码器
给定一个包含n个token的句子s ={W1, w2..，Wn },接入多层的Transformer，使用最后一层的隐藏向量作为token表示。此外，一个结点v ∈ Vg可能对应一个多token文本区间，在预训练中结点需要一个统一的表示。插入两个特殊的标记符[E1]和[E1]为区间的开始和结束。使用[E1]的隐藏问量作为结点v的区间表示xv,对于不同的结点使用不同的标记符对。从训练过的通用预训练模型开始预训练，以获得通用的语言理解能力。



###### 4.2.2 触发词-论元对辨别
设计触发词-论元对的辨别作为事件语义预训练的对比预训练任务，基本思想是学习相同事件的单词比不相关单词有更近的表示。可以注意到AMR结构是完全相似事件中的触发词-论元对，因此可以使用这些单词对作为正样本，训练文本编码器从负样本中辨别它们。
<br>
设![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE3.png) 为句子s中触发词-论元对的正样本集合,其中Rp={ARG, time, location}。对于一个具体的正样本对(t,a) ∈ Ps ,通过触发词替换和论元替换构建它对应的负样本。
* 在触发词替换阶段，通过随机采样mt个负样本触发词t∈Vs,和正样本论元a，构建mt个负样本对。负样本触发词t没有到a的有向边(ARG, time,location)。
* 在论元替换阶段，通过随机采样ma个负样本论元a∈Vs,构建ma个负样本对。
<br>
一个正样本对(t,a)的损失函数如下，其中W是一个学习相似性度量的可训练矩阵。
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE4.png)
<br>
一个mini-batch Bs的损失函数如下：
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE5.png)
<br>


#### 4.3 事件结构预训练
先前的工作展示事件相关的结构可以帮助抽取新的事件和发现、生成新的事件模式。因此，构建图结构预训练，使用GNN作为图编码器，学习可转移的事件相关的结构表示。具体地，在AMR子图辨别任务上预训练图编码器。

##### 4.3.1 图编码器
给一个图g，图编码器表示它为g =g(g,Xv),其中g为图编码器，{Xv}为接入图编码器的初始结点表示。CLEVE对图编码器的具体结构是无关的，因此使用sota的GNN模型，Graph lsomorphism Network。使用预训练文本编码器产生的对应文本区间的表示{Xv}作为图编码器的初始结点表示。这种节点初始化也隐式地对齐了CLEVE中事件语义和结构表示的语义空间，从而可以使它们更好地协作。

##### 4.3.2 AMR子图辨别
基本思想是通过将它们与从其他AMR图采样的子图区分开来学习从同一AMR图采样的子图的相似表示。给定M个AMR图g1,g2,..,gm,每个图对应无监督语料库中的一个句子，对于第i个图gi,从中随机采样两个子图得到一个正样本对a(2i-1)和a(2i),从mini-batch中其他AMR图采样的其他子图作为负样本，使用图编码器表示样本ai= g(ai,Xv)，损失函数如下:
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE6.png)
<br>

## 5．实验
在监督的事件抽取和无监督自由的事件抽取中评测模型
#### 5.1 预训练设置
使用New York Times语料作为CLEVE的无监督预训练语料，为了防止数据泄露，从NYT语料库中移除ACE 2005的全部文章。 文本编码器使用RoBERTa，从发布的checkpoint开始事件语义预训练。图编码器使用graph isomorphism network，从头开始预训练。
#### 5.2 CLEVE的改写
因为当前任务集中于预训练而不是对于事件抽取的微调，使用简单和通用的技术使预先训练的CLEVE适应下游的事件抽取任务(获得词表示然后分类)。
* 在监督情况下，对文本编码器使用动态多池化机制和使用图编码器编码相应的局部子图，结合这两种表示作为特征，在监督数据集上微调CLEVE。
* 在无监督自由情况下，直接使用预训练的CLEVE生成的表示作为所需的触发器/论元语义表示和事件结构表示。
#### 5.3 监督事件抽取
数据集使用ACE 2005和MAVEN，MAVEN仅能评测事件检测。在两个子任务上评估事件抽取的表现：
事件检测(ED)和事件论元抽取(EAE)。实验结果如下：
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE7.png)
<br>

#### 5.4 无监督自由的事件抽取
在无监督情形下，在ACE 2005和MAVEN上使用客观的自动度量和人工评估对CLEVE进行评估。
* 对于自动度量，使用外部聚类评估指标：B-Cubed Metrics，包括B-Cuded P、R和F1。B-Cubed Metrics通过将聚类结果与真实标准注释进行比较来评估聚类结果的质量。
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE8.png)
<br>
* 对于人工评估，邀请一名专家检查模型的输出，评估提取的事件是否完整且正确聚集，以及是否发现文本中的所有事件。人工评测的结果如下图
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE9.png)
<br>
通过在不同比例的随机采样MAVEN训练数据上比较事件检测效果，实验结果如下图，可以发现CLEVE对低资源的事件抽取任务有很大帮助。
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE10.png)
<br>
不同AMR解析器的实验结果如下，可以发现一个更好的AMR解析器可以带来更好的事件抽取表现，但是，这些提升不如相应的AMR性能改进那么显著，这表明CLEVE通常对AMR解析中的错误具有鲁棒性。
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE11.png)
<br>
在NYT和英文Wikipedia进行预训练，CLEVE的监督事件抽取表现如下，可以发现在相似领域上(ACE 2005对应NYT，MAVEN对应Wikipedia)预训练会使CLEVE在相应的数据集上受益。在ACE 2005上，尽管Wikipedia是NYT的2.28倍,但对其进行预训练的CLEVE表现不如在NYT上进行预训练的CLEVE(均在新闻领域)。可以看到领域内的收益主要来自事件语义而不是CLEVE框架中的结构，这表明可以为CLEVE开发专注于语义的域适应技术。
<br><br>
![数据集](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/CLEVE12.png)
<br>

## 6. 总结
* 提出CLEVE，一个事件抽取的对比预训练框架，利用大型无监督数据中丰富的事件知识。
* 在监督和无监督自由的事件抽取中，均得到了显著地提升。