# 阅读《Joint Event Extraction with Hierarchical Policy Network》：

## 1．动机
目前pipeline和joint的事件抽取，都存在冗余的实体-事件对信息，从而带来可能的错误。同时存在错误匹配问题(一个句子中存在多个事件)

## 2. 创新
* 使用policy network做事件抽取
* 提出了一个分等级(两个)的结构进行联合事件抽取，充分探索了事件抽取子任务之间的深层信息交互，并解决了多个事件和错误匹配问题。

## 3. 方法
强化学习包含三个主要概念，环境状态(Environment State)，行动(Action),奖励(Reward)。强化学习的目标是获得最多的累计奖励。
<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo2.png)
<br>
模型的整体框架如上图所示，主要分为两个部分：
* 事件级别Policy Network：首先计算每个token的状态，然后进行选择，最后计算奖励分数
由于NA事件类型不具有语义连续性，因此不对NA类型的编码进行学习，Loss为交叉熵，具体公式如下：，当全部事件级别的事件选择完成，整个句子的奖励分数为F1(S),具体公式如下：

<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo1.png)
<br>
* 论元级别Policy Network：大致流程和事件级的网络一致，其中定义了事件检索表，可以根据事件的类型压缩论元角色的集合，具体公式如下。

<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo3.png)
<br>
当前事件状态下全部论元级别的选择完成，最终的奖励分数为：
<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo4.png)
<br>
Loss函数如下，对奖励进行累加：
<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo5.png)
<br>
使用policy gradient method(参考链接)和REIN FORCE algorithm进行优化：
<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo6.png)
<br>

## 4. 实验
在ACE2005和TAC2015数据集上的实验效果如下：(使用Stanford CoreNLP toolkit处理数据)

<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo7.png)
<br>
在多句子情况下的实验效果：
<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo8.png)
<br>
在触发词和论元都存在出现在训练集和测试集的对比实验效果：
<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo9.png)
<br>
触发词分类的实验效果与触发词识别接近，进行错误分析的结果如下，
<br><br>
![模型在时空复杂度上的比较](https://raw.githubusercontent.com/Gun-God/PublicRes/main/img/jo10.png)
<br>

## 5. 总结
使用强化学习进行事件抽取，可以解决句子中多事件的重叠问题。